import pandas as pd
import numpy as np
import json
import os
import re
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
import pickle

# Create directories for AI models and outputs
os.makedirs('data/ai_models', exist_ok=True)
os.makedirs('data/ai_outputs', exist_ok=True)

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')

class AdvancedAnalytics:
    def __init__(self):
        """Initialize the advanced analytics module"""
        print("Initializing Advanced AI Analytics Module...")
        self.sia = SentimentIntensityAnalyzer()
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
        # Load data
        self.load_data()
        
    def load_data(self):
        """Load the data generated by the basic monitoring script"""
        print("Loading monitoring data...")
        
        self.data = {
            'news': None,
            'facebook': None,
            'twitter': None,
            'member_mentions': None,
            'topic_mentions': None,
            'alerts': None,
            'sentiment_trends': None,
            'member_profiles': None
        }
        
        # Check if files exist and load them
        if os.path.exists('data/news/news_data.json'):
            with open('data/news/news_data.json', 'r') as f:
                self.data['news'] = json.load(f)
        
        if os.path.exists('data/facebook/facebook_data.json'):
            with open('data/facebook/facebook_data.json', 'r') as f:
                self.data['facebook'] = json.load(f)
        
        if os.path.exists('data/twitter/twitter_data.json'):
            with open('data/twitter/twitter_data.json', 'r') as f:
                self.data['twitter'] = json.load(f)
        
        if os.path.exists('data/member_mentions.json'):
            with open('data/member_mentions.json', 'r') as f:
                self.data['member_mentions'] = json.load(f)
        
        if os.path.exists('data/topic_mentions.json'):
            with open('data/topic_mentions.json', 'r') as f:
                self.data['topic_mentions'] = json.load(f)
        
        if os.path.exists('data/alerts.json'):
            with open('data/alerts.json', 'r') as f:
                self.data['alerts'] = json.load(f)
        
        if os.path.exists('data/sentiment_trends.json'):
            with open('data/sentiment_trends.json', 'r') as f:
                self.data['sentiment_trends'] = json.load(f)
        
        if os.path.exists('data/member_profiles.json'):
            with open('data/member_profiles.json', 'r') as f:
                self.data['member_profiles'] = json.load(f)
        
        # Check if data is available, if not, run the basic monitoring script
        if not all([self.data['news'], self.data['facebook'], self.data['twitter']]):
            print("Running basic monitoring script to generate data...")
            import monitor
            monitor.run_monitoring()
            
            # Run prepare_data script to generate member profiles
            import prepare_data
            prepare_data.prepare_dashboard_data()
            
            # Reload the data
            self.load_data()
        
        print("Data loaded successfully")
    
    def preprocess_text(self, text):
        """Preprocess text for NLP tasks"""
        # Convert to lowercase
        text = text.lower()
        
        # Remove special characters and numbers
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # Tokenize
        tokens = word_tokenize(text)
        
        # Remove stopwords and lemmatize
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]
        
        return ' '.join(tokens)
    
    def advanced_sentiment_analysis(self):
        """Perform advanced sentiment analysis using VADER and custom rules"""
        print("Performing advanced sentiment analysis...")
        
        # Combine all content for analysis
        all_content = []
        
        # Process news content
        if self.data['news']:
            for item in self.data['news']:
                processed_text = self.preprocess_text(item['content'])
                sentiment = self.sia.polarity_scores(item['content'])
                
                # Update with more accurate sentiment
                item['sentiment'] = sentiment['compound'] * 0.5 + 0.5  # Convert to 0-1 scale
                item['sentiment_details'] = sentiment
                item['processed_text'] = processed_text
                
                all_content.append({
                    'source': 'news',
                    'content': item['content'],
                    'processed_text': processed_text,
                    'sentiment': item['sentiment'],
                    'sentiment_details': sentiment,
                    'member': item['member'],
                    'topic': item['topic'],
                    'date': item['date']
                })
        
        # Process Facebook content
        if self.data['facebook']:
            for post in self.data['facebook']:
                processed_text = self.preprocess_text(post['content'])
                sentiment = self.sia.polarity_scores(post['content'])
                
                # Update with more accurate sentiment
                post['sentiment'] = sentiment['compound'] * 0.5 + 0.5  # Convert to 0-1 scale
                post['sentiment_details'] = sentiment
                post['processed_text'] = processed_text
                
                all_content.append({
                    'source': 'facebook_post',
                    'content': post['content'],
                    'processed_text': processed_text,
                    'sentiment': post['sentiment'],
                    'sentiment_details': sentiment,
                    'member': post['member'],
                    'topic': post['topic'],
                    'date': post['date']
                })
                
                # Process comments
                for comment in post['comments']:
                    processed_text = self.preprocess_text(comment['content'])
                    sentiment = self.sia.polarity_scores(comment['content'])
                    
                    # Update with more accurate sentiment
                    comment['sentiment'] = sentiment['compound'] * 0.5 + 0.5  # Convert to 0-1 scale
                    comment['sentiment_details'] = sentiment
                    comment['processed_text'] = processed_text
                    
                    all_content.append({
                        'source': 'facebook_comment',
                        'content': comment['content'],
                        'processed_text': processed_text,
                        'sentiment': comment['sentiment'],
                        'sentiment_details': sentiment,
                        'member': post['member'],  # Associate with the post's member
                        'topic': post['topic'],    # Associate with the post's topic
                        'date': comment['date']
                    })
        
        # Process Twitter content
        if self.data['twitter']:
            for tweet in self.data['twitter']:
                processed_text = self.preprocess_text(tweet['content'])
                sentiment = self.sia.polarity_scores(tweet['content'])
                
                # Update with more accurate sentiment
                tweet['sentiment'] = sentiment['compound'] * 0.5 + 0.5  # Convert to 0-1 scale
                tweet['sentiment_details'] = sentiment
                tweet['processed_text'] = processed_text
                
                all_content.append({
                    'source': 'twitter_tweet',
                    'content': tweet['content'],
                    'processed_text': processed_text,
                    'sentiment': tweet['sentiment'],
                    'sentiment_details': sentiment,
                    'member': tweet['member'],
                    'topic': tweet['topic'],
                    'date': tweet['date']
                })
                
                # Process replies
                for reply in tweet['replies']:
                    processed_text = self.preprocess_text(reply['content'])
                    sentiment = self.sia.polarity_scores(reply['content'])
                    
                    # Update with more accurate sentiment
                    reply['sentiment'] = sentiment['compound'] * 0.5 + 0.5  # Convert to 0-1 scale
                    reply['sentiment_details'] = sentiment
                    reply['processed_text'] = processed_text
                    
                    all_content.append({
                        'source': 'twitter_reply',
                        'content': reply['content'],
                        'processed_text': processed_text,
                        'sentiment': reply['sentiment'],
                        'sentiment_details': sentiment,
                        'member': tweet['member'],  # Associate with the tweet's member
                        'topic': tweet['topic'],    # Associate with the tweet's topic
                        'date': reply['date']
                    })
        
        # Save all content to a DataFrame for further analysis
        self.content_df = pd.DataFrame(all_content)
        
        # Save to CSV for reference
        self.content_df.to_csv('data/ai_outputs/all_content_with_sentiment.csv', index=False)
        
        print("Advanced sentiment analysis completed")
        
        # Update the JSON files with the new sentiment scores
        self.save_updated_data()
        
        return self.content_df
    
    def save_updated_data(self):
        """Save the updated data with advanced sentiment analysis"""
        # Save news data
        if self.data['news']:
            with open('data/news/news_data_advanced.json', 'w') as f:
                json.dump(self.data['news'], f, indent=2)
        
        # Save Facebook data
        if self.data['facebook']:
            with open('data/facebook/facebook_data_advanced.json', 'w') as f:
                json.dump(self.data['facebook'], f, indent=2)
        
        # Save Twitter data
        if self.data['twitter']:
            with open('data/twitter/twitter_data_advanced.json', 'w') as f:
                json.dump(self.data['twitter'], f, indent=2)
    
    def topic_modeling(self, num_topics=10):
        """Perform topic modeling using LDA"""
        print(f"Performing topic modeling with {num_topics} topics...")
        
        if not hasattr(self, 'content_df'):
            self.advanced_sentiment_analysis()
        
        # Combine all processed text
        all_text = self.content_df['processed_text'].tolist()
        
        # Create a document-term matrix
        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
        dtm = vectorizer.fit_transform(all_text)
        
        # Create and fit the LDA model
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
        lda.fit(dtm)
        
        # Get feature names (words)
        feature_names = vectorizer.get_feature_names_out()
        
        # Extract top words for each topic
        topics = []
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words
            top_words = [feature_names[i] for i in top_words_idx]
            
            topics.append({
                'id': topic_idx,
                'top_words': top_words,
                'weight': float(topic.sum())  # Convert numpy float to Python float for JSON serialization
            })
        
        # Save topics to JSON
        with open('data/ai_outputs/lda_topics.json', 'w') as f:
            json.dump(topics, f, indent=2)
        
        # Assign topics to content
        doc_topics = lda.transform(dtm)
        self.content_df['dominant_topic'] = doc_topics.argmax(axis=1)
        
        # Save the updated DataFrame
        self.content_df.to_csv('data/ai_outputs/all_content_with_topics.csv', index=False)
        
        # Generate topic word clouds
        self.generate_topic_wordclouds(lda, feature_names)
        
        print("Topic modeling completed")
        return topics
    
    def generate_topic_wordclouds(self, lda_model, feature_names):
        """Generate word clouds for each topic"""
        print("Generating topic word clouds...")
        
        # Create directory for word clouds
        os.makedirs('data/ai_outputs/wordclouds', exist_ok=True)
        
        for topic_idx, topic in enumerate(lda_model.components_):
            # Get the weights for all words in the topic
            word_weights = {feature_names[i]: topic[i] for i in range(len(feature_names))}
            
            # Generate word cloud
            wordcloud = WordCloud(
                width=800, height=400,
                background_color='white',
                max_words=50
            ).generate_from_frequencies(word_weights)
            
            # Save the word cloud
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'Topic {topic_idx + 1}')
            plt.tight_layout()
            plt.savefig(f'data/ai_outputs/wordclouds/topic_{topic_idx + 1}.png')
            plt.close()
    
    def entity_network_analysis(self):
        """Analyze relationships between parliament members and topics"""
        print("Performing entity network analysis...")
        
        if not hasattr(self, 'content_df'):
            self.advanced_sentiment_analysis()
        
        # Create member-topic matrix
        member_topic_df = self.content_df.groupby(['member', 'topic']).agg({
            'sentiment': 'mean',
            'content': 'count'
        }).reset_index()
        
        member_topic_df.rename(columns={'content': 'mention_count'}, inplace=True)
        
        # Save to CSV
        member_topic_df.to_csv('data/ai_outputs/member_topic_network.csv', index=False)
        
        # Create member-member co-occurrence matrix
        members = self.content_df['member'].unique()
        member_cooccurrence = pd.DataFrame(0, index=members, columns=members)
        
        # Group content by source (e.g., news article, Facebook post)
        for source_type in ['news', 'facebook_post', 'twitter_tweet']:
            source_content = self.content_df[self.content_df['source'] == source_type]
            
            # Group by date to find co-occurrences
            for date, group in source_content.groupby('date'):
                mentioned_members = group['member'].unique()
                
                # If multiple members are mentioned in the same content
                if len(mentioned_members) > 1:
                    for i in range(len(mentioned_members)):
                        for j in range(i+1, len(mentioned_members)):
                            member_a = mentioned_members[i]
                            member_b = mentioned_members[j]
                            member_cooccurrence.loc[member_a, member_b] += 1
                            member_cooccurrence.loc[member_b, member_a] += 1
        
        # Save to CSV
        member_cooccurrence.to_csv('data/ai_outputs/member_cooccurrence.csv')
        
        # Create network visualization data
        network_data = {
            'nodes': [],
            'links': []
        }
        
        # Add member nodes
        for member in members:
            network_data['nodes'].append({
                'id': member,
                'type': 'member',
                'mentions': int(self.content_df[self.content_df['member'] == member].shape[0])
            })
        
        # Add topic nodes
        topics = self.content_df['topic'].unique()
        for topic in topics:
            network_data['nodes'].append({
                'id': topic,
                'type': 'topic',
                'mentions': int(self.content_df[self.content_df['topic'] == topic].shape[0])
            })
        
        # Add member-topic links
        for _, row in member_topic_df.iterrows():
            network_data['links'].append({
                'source': row['member'],
                'target': row['topic'],
                'value': int(row['mention_count']),
                'sentiment': float(row['sentiment'])
            })
        
        # Add member-member links
        for member_a in members:
            for member_b in members:
                if member_a != member_b and member_cooccurrence.loc[member_a, member_b] > 0:
                    network_data['links'].append({
                        'source': member_a,
                        'target': member_b,
                        'value': int(member_cooccurrence.loc[member_a, member_b]),
                        'sentiment': 0.5  # Neutral by default
                    })
        
        # Save network data to JSON
        with open('data/ai_outputs/entity_network.json', 'w') as f:
            json.dump(network_data, f, indent=2)
        
        print("Entity network analysis completed")
        return network_data
    
    def trend_prediction(self, days_to_predict=7):
        """Predict sentiment trends for the next few days"""
        print(f"Predicting sentiment trends for the next {days_to_predict} days...")
        
        if not self.data['sentiment_trends']:
            print("No sentiment trend data available for prediction")
            return None
        
        # Convert sentiment trends to DataFrame
        trends_df = pd.DataFrame(self.data['sentiment_trends'])
        
        # Ensure dates are in datetime format
        trends_df['date'] = pd.to_datetime(trends_df['date'])
        
        # Sort by date
        trends_df = trends_df.sort_values('date')
        
        # Extract features for prediction
        X = np.array(range(len(trends_df))).reshape(-1, 1)
        y_overall = trends_df['overall'].values
        y_news = trends_df['news'].values
        y_facebook = trends_df['facebook'].values
        y_twitter = trends_df['twitter'].values
        
        # Create prediction model (simple linear regression for demo)
        from sklearn.linear_model import LinearRegression
        
        # Train models
        model_overall = LinearRegression().fit(X, y_overall)
        model_news = LinearRegression().fit(X, y_news)
        model_facebook = LinearRegression().fit(X, y_facebook)
        model_twitter = LinearRegression().fit(X, y_twitter)
        
        # Generate future dates
        last_date = trends_df['date'].max()
        future_dates = [last_date + timedelta(days=i+1) for i in range(days_to_predict)]
        
        # Predict future values
        X_future = np.array(range(len(trends_df), len(trends_df) + days_to_predict)).reshape(-1, 1)
        
        predictions = []
        for i, date in enumerate(future_dates):
            # Make predictions
            overall_pred = float(model_overall.predict(X_future[i:i+1])[0])
            news_pred = float(model_news.predict(X_future[i:i+1])[0])
            facebook_pred = float(model_facebook.predict(X_future[i:i+1])[0])
            twitter_pred = float(model_twitter.predict(X_future[i:i+1])[0])
            
            # Ensure predictions are within valid range (0-1)
            overall_pred = max(0, min(1, overall_pred))
            news_pred = max(0, min(1, news_pred))
            facebook_pred = max(0, min(1, facebook_pred))
            twitter_pred = max(0, min(1, twitter_pred))
            
            # Create prediction object
            prediction = {
                'date': date.strftime('%Y-%m-%d'),
                'overall': overall_pred,
                'news': news_pred,
                'facebook': facebook_pred,
                'twitter': twitter_pred,
                'predicted': True
            }
            
            predictions.append(prediction)
        
        # Save predictions to JSON
        with open('data/ai_outputs/sentiment_predictions.json', 'w') as f:
            json.dump(predictions, f, indent=2)
        
        # Create visualization of predictions
        self.visualize_predictions(trends_df, predictions)
        
        print("Trend prediction completed")
        return predictions
    
    def visualize_predictions(self, historical_df, predictions):
        """Visualize historical data and predictions"""
        # Convert predictions to DataFrame
        predictions_df = pd.DataFrame(predictions)
        predictions_df['date'] = pd.to_datetime(predictions_df['date'])
        
        # Combine historical and predicted data
        historical_df['predicted'] = False
        combined_df = pd.concat([historical_df, predictions_df])
        
        # Create visualization
        plt.figure(figsize=(12, 6))
        
        # Plot historical data
        plt.plot(historical_df['date'], historical_df['overall'] * 100, 'o-', label='Historical Overall', linewidth=2)
        plt.plot(historical_df['date'], historical_df['news'] * 100, 's-', label='Historical News')
        plt.plot(historical_df['date'], historical_df['facebook'] * 100, '^-', label='Historical Facebook')
        plt.plot(historical_df['date'], historical_df['twitter'] * 100, 'd-', label='Historical Twitter')
        
        # Plot predicted data
        plt.plot(predictions_df['date'], predictions_df['overall'] * 100, 'o--', label='Predicted Overall', linewidth=2)
        plt.plot(predictions_df['date'], predictions_df['news'] * 100, 's--', label='Predicted News')
        plt.plot(predictions_df['date'], predictions_df['facebook'] * 100, '^--', label='Predicted Facebook')
        plt.plot(predictions_df['date'], predictions_df['twitter'] * 100, 'd--', label='Predicted Twitter')
        
        # Add vertical line to separate historical and predicted data
        plt.axvline(x=historical_df['date'].max(), color='gray', linestyle='--')
        
        plt.title('Sentiment Trend Prediction')
        plt.xlabel('Date')
        plt.ylabel('Sentiment Score (%)')
        plt.ylim(30, 80)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.tight_layout()
        
        plt.savefig('data/ai_outputs/sentiment_prediction.png')
        plt.close()
    
    def build_sentiment_classifier(self):
        """Build and train a deep learning sentiment classifier"""
        print("Building sentiment classifier model...")
        
        if not hasattr(self, 'content_df'):
            self.advanced_sentiment_analysis()
        
        # Prepare data
        texts = self.content_df['content'].tolist()
        
        # Convert sentiment scores to classes (positive, neutral, negative)
        sentiments = []
        for sentiment in self.content_df['sentiment']:
            if sentiment > 0.6:
                sentiments.append(2)  # Positive
            elif sentiment > 0.4:
                sentiments.append(1)  # Neutral
            else:
                sentiments.append(0)  # Negative
        
        # Tokenize texts
        tokenizer = Tokenizer(num_words=10000)
        tokenizer.fit_on_texts(texts)
        sequences = tokenizer.texts_to_sequences(texts)
        
        # Pad sequences
        max_length = 100
        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')
        
        # Split data
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            padded_sequences, sentiments, test_size=0.2, random_state=42
        )
        
        # Convert to numpy arrays
        y_train = np.array(y_train)
        y_test = np.array(y_test)
        
        # Build model
        model = Sequential()
        model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))
        model.add(Bidirectional(LSTM(64, return_sequences=True)))
        model.add(Bidirectional(LSTM(32)))
        model.add(Dense(64, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive
        
        # Compile model
        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        
        # Train model
        history = model.fit(
            X_train, y_train,
            epochs=5,
            batch_size=32,
            validation_data=(X_test, y_test),
            verbose=1
        )
        
        # Evaluate model
        loss, accuracy = model.evaluate(X_test, y_test)
        print(f"Model accuracy: {accuracy:.4f}")
        
        # Save model and tokenizer
        model.save('data/ai_models/sentiment_classifier')
        
        with open('data/ai_models/tokenizer.pickle', 'wb') as handle:
            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
        
        # Save model info
        model_info = {
            'accuracy': float(accuracy),
            'loss': float(loss),
            'max_length': max_length,
            'classes': ['negative', 'neutral', 'positive']
        }
        
        with open('data/ai_models/model_info.json', 'w') as f:
            json.dump(model_info, f, indent=2)
        
        print("Sentiment classifier built and saved")
        return model, tokenizer
    
    def generate_member_insights(self):
        """Generate AI-powered insights for each parliament member"""
        print("Generating member insights...")
        
        if not hasattr(self, 'content_df'):
            self.advanced_sentiment_analysis()
        
        member_insights = {}
        
        for member in self.content_df['member'].unique():
            # Filter content for this member
            member_content = self.content_df[self.content_df['member'] == member]
            
            # Calculate sentiment statistics
            sentiment_mean = member_content['sentiment'].mean()
            sentiment_std = member_content['sentiment'].std()
            
            # Calculate sentiment by source
            sentiment_by_source = member_content.groupby('source')['sentiment'].mean().to_dict()
            
            # Get most common topics
            if 'topic' in member_content.columns and 'dominant_topic' not in member_content.columns:
                top_topics = member_content['topic'].value_counts().head(3).to_dict()
            elif 'dominant_topic' in member_content.columns:
                top_topics = member_content['dominant_topic'].value_counts().head(3).to_dict()
                # Convert numpy int64 to Python int for JSON serialization
                top_topics = {int(k): int(v) for k, v in top_topics.items()}
            else:
                top_topics = {}
            
            # Calculate sentiment trend
            member_content['date'] = pd.to_datetime(member_content['date'])
            sentiment_trend = member_content.sort_values('date').groupby(member_content['date'].dt.date)['sentiment'].mean()
            
            # Convert to list of (date, sentiment) tuples
            sentiment_trend_list = [(str(date), float(sentiment)) for date, sentiment in sentiment_trend.items()]
            
            # Generate insights
            insights = []
            
            # Insight 1: Overall sentiment
            if sentiment_mean > 0.6:
                insights.append("Overall positive public perception")
            elif sentiment_mean < 0.4:
                insights.append("Facing significant negative sentiment")
            else:
                insights.append("Mixed public perception")
            
            # Insight 2: Sentiment volatility
            if sentiment_std > 0.2:
                insights.append("High volatility in public sentiment")
            elif sentiment_std < 0.1:
                insights.append("Consistent public perception")
            
            # Insight 3: Platform-specific insights
            for source, sentiment in sentiment_by_source.items():
                if 'news' in source and sentiment > 0.6:
                    insights.append("Strong positive coverage in news media")
                elif 'news' in source and sentiment < 0.4:
                    insights.append("Negative portrayal in news media")
                
                if 'facebook' in source and sentiment > 0.6:
                    insights.append("Popular on Facebook")
                elif 'facebook' in source and sentiment < 0.4:
                    insights.append("Facing criticism on Facebook")
                
                if 'twitter' in source and sentiment > 0.6:
                    insights.append("Positive Twitter engagement")
                elif 'twitter' in source and sentiment < 0.4:
                    insights.append("Negative Twitter sentiment")
            
            # Insight 4: Trend direction
            if len(sentiment_trend_list) >= 2:
                first_sentiment = sentiment_trend_list[0][1]
                last_sentiment = sentiment_trend_list[-1][1]
                
                if last_sentiment > first_sentiment + 0.1:
                    insights.append("Improving public perception")
                elif last_sentiment < first_sentiment - 0.1:
                    insights.append("Declining public sentiment")
            
            # Store all insights
            member_insights[member] = {
                'sentiment_mean': float(sentiment_mean),
                'sentiment_std': float(sentiment_std),
                'sentiment_by_source': {k: float(v) for k, v in sentiment_by_source.items()},
                'top_topics': top_topics,
                'sentiment_trend': sentiment_trend_list,
                'insights': insights
            }
        
        # Save insights to JSON
        with open('data/ai_outputs/member_insights.json', 'w') as f:
            json.dump(member_insights, f, indent=2)
        
        print("Member insights generated")
        return member_insights
    
    def generate_chatbot_data(self):
        """Generate data for the AI chatbot feature"""
        print("Generating chatbot training data...")
        
        # Create sample QA pairs for the chatbot
        qa_pairs = [
            {
                "question": "What is the overall sentiment about the Parliament of Mongolia?",
                "answer": "Based on our analysis, the overall sentiment about the Parliament of Mongolia is {sentiment}. This is calculated from {count} mentions across news, Facebook, and Twitter."
            },
            {
                "question": "Who is the most mentioned parliament member?",
                "answer": "The most mentioned parliament member is {member} with {count} mentions across all platforms."
            },
            {
                "question": "What is the sentiment about {member}?",
                "answer": "The sentiment about {member} is {sentiment}. This is based on {count} mentions across news, Facebook, and Twitter."
            },
            {
                "question": "What are the top topics discussed about the Parliament?",
                "answer": "The top topics discussed about the Parliament are: {topics}."
            },
            {
                "question": "What is the sentiment trend for the Parliament?",
                "answer": "The sentiment trend for the Parliament shows {trend}. {additional_info}"
            },
            {
                "question": "Which platform has the most positive sentiment?",
                "answer": "Based on our analysis, {platform} has the most positive sentiment about the Parliament with a score of {score}."
            },
            {
                "question": "What are the recent alerts about the Parliament?",
                "answer": "There are {count} recent alerts about the Parliament. The most important ones are: {alerts}."
            },
            {
                "question": "How is the sentiment about {topic}?",
                "answer": "The sentiment about {topic} is {sentiment}. This is based on {count} mentions across all platforms."
            },
            {
                "question": "Compare the sentiment between {member1} and {member2}",
                "answer": "{member1} has a sentiment score of {score1}, while {member2} has a sentiment score of {score2}. {comparison}"
            },
            {
                "question": "What are the predictions for sentiment in the coming days?",
                "answer": "Our AI model predicts that the overall sentiment will {prediction} in the coming days. {details}"
            }
        ]
        
        # Save QA pairs to JSON
        with open('data/ai_outputs/chatbot_qa_pairs.json', 'w') as f:
            json.dump(qa_pairs, f, indent=2)
        
        # Create a simple response generator function
        def generate_response(question, data):
            """Generate a response based on the question and available data"""
            # This is a simplified version for the demo
            # In a real implementation, this would use NLP to understand the question
            # and generate an appropriate response
            
            response = "I don't have enough information to answer that question."
            
            # Check for sentiment questions
            if "overall sentiment" in question.lower():
                if data['sentiment_trends']:
                    overall = data['sentiment_trends'][0]['overall']
                    sentiment_text = "positive" if overall > 0.6 else "neutral" if overall > 0.4 else "negative"
                    count = len(data['news']) + len(data['facebook']) + len(data['twitter'])
                    response = f"Based on our analysis, the overall sentiment about the Parliament of Mongolia is {sentiment_text} ({overall:.2f}). This is calculated from {count} mentions across news, Facebook, and Twitter."
            
            # Check for member questions
            elif "most mentioned" in question.lower() and "member" in question.lower():
                if data['member_mentions']:
                    # Find member with most mentions
                    most_mentioned = max(data['member_mentions'].items(), key=lambda x: x[1]['total'])
                    member_name = most_mentioned[0]
                    mention_count = most_mentioned[1]['total']
                    response = f"The most mentioned parliament member is {member_name} with {mention_count} mentions across all platforms."
            
            # Check for topic questions
            elif "top topics" in question.lower():
                if data['topic_mentions']:
                    # Sort topics by total mentions
                    sorted_topics = sorted(data['topic_mentions'].items(), key=lambda x: x[1]['total'], reverse=True)
                    top_topics = [topic[0] for topic in sorted_topics[:3]]
                    response = f"The top topics discussed about the Parliament are: {', '.join(top_topics)}."
            
            return response
        
        # Save a sample of generated responses
        sample_responses = []
        for qa_pair in qa_pairs:
            question = qa_pair['question']
            generated_response = generate_response(question, self.data)
            
            sample_responses.append({
                "question": question,
                "template_answer": qa_pair['answer'],
                "generated_answer": generated_response
            })
        
        with open('data/ai_outputs/chatbot_sample_responses.json', 'w') as f:
            json.dump(sample_responses, f, indent=2)
        
        print("Chatbot data generated")
        return qa_pairs
    
    def run_all_analyses(self):
        """Run all advanced AI analyses"""
        print("Running all advanced AI analyses...")
        
        # 1. Advanced sentiment analysis
        self.advanced_sentiment_analysis()
        
        # 2. Topic modeling
        self.topic_modeling()
        
        # 3. Entity network analysis
        self.entity_network_analysis()
        
        # 4. Trend prediction
        self.trend_prediction()
        
        # 5. Build sentiment classifier
        self.build_sentiment_classifier()
        
        # 6. Generate member insights
        self.generate_member_insights()
        
        # 7. Generate chatbot data
        self.generate_chatbot_data()
        
        print("All advanced AI analyses completed")
        
        # Update dashboard data with AI outputs
        self.update_dashboard_data()
    
    def update_dashboard_data(self):
        """Update dashboard data with AI outputs"""
        print("Updating dashboard data with AI outputs...")
        
        # Create public data directory if it doesn't exist
        os.makedirs('public/data', exist_ok=True)
        
        # Copy AI outputs to public directory
        
        # 1. Member insights
        if os.path.exists('data/ai_outputs/member_insights.json'):
            with open('data/ai_outputs/member_insights.json', 'r') as f:
                member_insights = json.load(f)
            
            with open('public/data/member_insights.json', 'w') as f:
                json.dump(member_insights, f, indent=2)
        
        # 2. Topic modeling results
        if os.path.exists('data/ai_outputs/lda_topics.json'):
            with open('data/ai_outputs/lda_topics.json', 'r') as f:
                topics = json.load(f)
            
            with open('public/data/ai_topics.json', 'w') as f:
                json.dump(topics, f, indent=2)
        
        # 3. Entity network
        if os.path.exists('data/ai_outputs/entity_network.json'):
            with open('data/ai_outputs/entity_network.json', 'r') as f:
                network = json.load(f)
            
            with open('public/data/entity_network.json', 'w') as f:
                json.dump(network, f, indent=2)
        
        # 4. Sentiment predictions
        if os.path.exists('data/ai_outputs/sentiment_predictions.json'):
            with open('data/ai_outputs/sentiment_predictions.json', 'r') as f:
                predictions = json.load(f)
            
            with open('public/data/sentiment_predictions.json', 'w') as f:
                json.dump(predictions, f, indent=2)
        
        # 5. Chatbot data
        if os.path.exists('data/ai_outputs/chatbot_qa_pairs.json'):
            with open('data/ai_outputs/chatbot_qa_pairs.json', 'r') as f:
                qa_pairs = json.load(f)
            
            with open('public/data/chatbot_data.json', 'w') as f:
                json.dump(qa_pairs, f, indent=2)
        
        print("Dashboard data updated with AI outputs")

# Run the advanced analytics if executed directly
if __name__ == "__main__":
    analytics = AdvancedAnalytics()
    analytics.run_all_analyses()
